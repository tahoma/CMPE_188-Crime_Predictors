{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# San Francisco Police Department Incident Dataset\n",
    "\n",
    "In this notebook, we'll be exploring the incident report dataset from the San Francisco Police Department using data mining and visualization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our raw source dataset can be retrieved from this link: <https://data.sfgov.org/api/views/tmnf-yvry/rows.csv?accessType=DOWNLOAD>.\n",
    "\n",
    "It is larger than the maximum 100 MB file size that github allows.  Until we decide how we're going to filter it and can check it in, just make certain it's downloaded into the same directory as the ipython notebook file.\n",
    "\n",
    "Once the dataset is in place, the following code can read in the CSV as a dataframe and report the number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incident_df = pandas.read_csv('./SFPD_Incidents_-_from_1_January_2003.csv')\n",
    "print(incident_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the entire list of features from original dataset so we can see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(incident_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is likely to prove useful to know the unique types within the Resolution, Category, and Descript columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution_types = incident_df.groupby('Resolution').Resolution.nunique()\n",
    "print(\"{} unique values in 'Resolution' column\".format(resolution_types.size))\n",
    "resolution_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_types = incident_df.groupby('Category').Category.nunique()\n",
    "print(\"{} unique values in 'Category' column\".format(category_types.size))\n",
    "category_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descript_types = incident_df.groupby('Descript').Descript.nunique()\n",
    "print(\"{} unique values in 'Descript' column\".format(descript_types.size))\n",
    "descript_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to confirm that we're getting the expected data, report the value in the 'Date' column for row 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = incident_df.get_value(0, 'Date')\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a neat way to perform a select to filter rows on a dataframe. Note that I'm creating a smaller dataframe by selecting the rows where the 'Date' column is equal to the date read above for row 0.  Be careful, though, and note that the row indices still match those from the original dataset, which will cause you fits if you're trying to iterate over row indices with the smaller dataset since it no longer has contiguous indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_on_date = incident_df[incident_df.Date == date]\n",
    "df_on_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Initializing Bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bokeh is a useful Python visualization library that we'll be using multiple times below.  Rather than repeat its initialization or spread it out over multiple locations in the document, I'm giving its initialization an independent section early in the notebook.\n",
    "\n",
    "Also, please note that I had to run the following command before I could get Bokeh plots to display at all.\n",
    "\n",
    "```\n",
    "jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bokeh for visualizations.\n",
    "import bokeh.io\n",
    "import bokeh.models\n",
    "import bokeh.plotting\n",
    "\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Mapping Incidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We probably need to be able to do visualizations involving\n",
    "layering over a base map of San Francisco based upon coordinates\n",
    "expressed like we have available in the dataset. To start with, \n",
    "I've been using Bokeh:\n",
    "<http://bokeh.pydata.org/en/latest/docs/user_guide/geo.html>.\n",
    "\n",
    "Given that it was needed for Bokeh mapping, I went ahead \n",
    "and grabbed a Google Maps API key for a project named\n",
    "\"CMPE 188 - Crime Predictors\".\n",
    "It is \"AIzaSyAs6Ugy0oz0R5YAxep9-kQ170t0U2fjELQ\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_incidents(df):\n",
    "\n",
    "    # Extract the coordinates for the incidents on the extracted date.\n",
    "    map_lats = []\n",
    "    map_lons = []\n",
    "    for row in df.itertuples():\n",
    "        map_lats.append(float(row.Y))\n",
    "        map_lons.append(float(row.X))\n",
    "    map_data = dict(lat=map_lats, lon=map_lons)\n",
    "    \n",
    "    # Create the map plot object.\n",
    "    map_plot = bokeh.models.GMapPlot(\n",
    "        x_range=bokeh.models.DataRange1d(),\n",
    "        y_range=bokeh.models.DataRange1d(),\n",
    "        map_options=bokeh.models.GMapOptions(\n",
    "            lat=map_lats[0],\n",
    "            lng=map_lons[0],\n",
    "            map_type=\"roadmap\",\n",
    "            zoom=11\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Give the map plot a title.\n",
    "    map_plot.title.text = \"San Francisco Police Department Incident Locations\"\n",
    "    \n",
    "    # Set the Google Maps API key to our project-specific key.\n",
    "    map_plot.api_key = \"AIzaSyAs6Ugy0oz0R5YAxep9-kQ170t0U2fjELQ\"\n",
    "    \n",
    "    # Add glyphs of blue circles at extracted locations of incidents.\n",
    "    map_plot.add_glyph(\n",
    "        bokeh.models.ColumnDataSource(data=map_data),\n",
    "        bokeh.models.Circle(\n",
    "            x=\"lon\",\n",
    "            y=\"lat\",\n",
    "            size=10,\n",
    "            fill_color=\"blue\",\n",
    "            fill_alpha=0.20,\n",
    "            line_color=None\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add the standard map control tools.\n",
    "    map_plot.add_tools(\n",
    "        bokeh.models.PanTool(),\n",
    "        bokeh.models.WheelZoomTool(),\n",
    "        bokeh.models.BoxSelectTool()\n",
    "    )\n",
    "\n",
    "    return map_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh.io.show(map_incidents(df_on_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Clustering Incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brief overview - narrowing down the sfpd dataset by given date \n",
    "# and other minor things to get small enough subset for K means algorithm to run on my hardware\n",
    "# for k means, the input must be numeric and the dataframe converted into a matrix to ditch the column headers \n",
    "\n",
    "# please remember, this is really only horseplay\n",
    "\n",
    "reduced_df = incident_df[['Category', 'Descript', 'DayOfWeek', 'Date', 'Time']]\n",
    "\n",
    "#select rows on only 1 day\n",
    "date = reduced_df.get_value(0, 'Date')\n",
    "\n",
    "#filter useless? information\n",
    "reduced_df = reduced_df[reduced_df.Category != \"FRAUD\"]\n",
    "reduced_df = reduced_df[reduced_df.Category != \"NON-CRIMINAL\"]\n",
    "reduced_df = reduced_df[reduced_df.Date == date]\n",
    "reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of different description cases out of each category, tells the number of \n",
    "# categorys but not neccesarily the diff descriptions since some are duplicates\n",
    "# found help from this site \n",
    "# http://stackoverflow.com/questions/15411158/pandas-countdistinct-equivalent\n",
    "\n",
    "# for a given day \n",
    "reduced_df.groupby('Category').Descript.nunique()\n",
    "\n",
    "\n",
    "# for the entire dataset\n",
    "incident_df.groupby('Category').Descript.nunique()\n",
    "\n",
    "\n",
    "#take a sample set of features to feed into Kmeans\n",
    "\n",
    "data_df_test = incident_df[['Category', 'Descript']]\n",
    "\n",
    "#data_df_test = data_df_test[data_df_test.Category != \"TREA\"]\n",
    "#data_df_test = data_df_test[data_df_test.Category != \"RECOVERED VEHICLE\"]\n",
    "#data_df_test = data_df_test[data_df_test.Category != \"PORNOGRAPHY/OBSCENE MAT\"]\n",
    "#data_df_test = data_df_test[data_df_test.Category != \"BRIBERY\"]\n",
    "#data_df_test = data_df_test[data_df_test.Category != \"SUICIDE\"]\n",
    "\n",
    "\n",
    "data_df_test.groupby(\"Category\").Descript.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# in order to use K-means, the inputs must be numerical, so we have to discretize the category input\n",
    "# found this post off stackoverflow helpful \n",
    "# http://stackoverflow.com/questions/34915813/convert-text-columns-into-numbers-in-sklearn \n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "test_series = reduced_df[reduced_df.columns[:]].apply(le.fit_transform)\n",
    "\n",
    "print(test_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the input data before using kmeans\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "normalized_df = (test_series-test_series.mean())/test_series.std()\n",
    "\n",
    "# remove the date and time columns since LabelEncoder zero'ed all date and times (not sure why)\n",
    "\n",
    "# TMT: They were zero'd because you were normalizing labels to numbers, and the dataframe\n",
    "# was already selected down incidents from the same Date (and, thus, DayOfWeek).\n",
    "\n",
    "del normalized_df[\"DayOfWeek\"]\n",
    "del normalized_df[\"Date\"]\n",
    "\n",
    "# remove the column names by transforming dataframe inot matrix\n",
    "\n",
    "testdata = normalized_df.as_matrix(columns=None)\n",
    "\n",
    "print(testdata)\n",
    "#perform k-means analysis on the reduced data set\n",
    "\n",
    "kmean = KMeans(n_clusters=5) \n",
    "\n",
    "kmean.fit(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = bokeh.plotting.figure(\n",
    "    width = 500,\n",
    "    height = 500,\n",
    "    title = 'CrimeStoppers',\n",
    "    x_axis_label = \"category\",\n",
    "    y_axis_label = \"descript\"\n",
    ")\n",
    "\n",
    "#plot centroid / cluster center / group mean for each group\n",
    "\n",
    "clus_xs = []\n",
    "\n",
    "clus_ys = []\n",
    "\n",
    "#we get the  cluster x / y values from the k-means algorithm\n",
    "\n",
    "for entry in kmean.cluster_centers_:\n",
    "\n",
    "   clus_xs.append(entry[0])\n",
    "\n",
    "   clus_ys.append(entry[1])\n",
    "\n",
    "#the cluster center is marked by a circle, with a cross in it\n",
    "\n",
    "plot.circle_cross(\n",
    "    x=clus_xs,\n",
    "    y=clus_ys,\n",
    "    size=40,\n",
    "    fill_alpha=0,\n",
    "    line_width=2,\n",
    "    color=['red', 'blue', 'purple', 'green', 'yellow']\n",
    ")\n",
    "\n",
    "plot.text(text = ['something', 'other', 'another', 'yet', 'more'], x=clus_xs, y=clus_ys, text_font_size='30pt')\n",
    "\n",
    "i = 0 #counter\n",
    "\n",
    "#begin plotting each petal length / width\n",
    "\n",
    "#We get our x / y values from the original plot data.\n",
    "\n",
    "#The k-means algorithm tells us which 'color' each plot point is,\n",
    "\n",
    "#and therefore which group it is a member of.\n",
    "\n",
    "for sample in testdata:\n",
    "\n",
    "    #\"labels_\" tells us which cluster each plot point is a member of\n",
    "    if kmean.labels_[i] == 0:\n",
    "        plot.circle(x=sample[0], y=sample[1], size=15, color=\"red\")\n",
    "    if kmean.labels_[i] == 1:\n",
    "        plot.circle(x=sample[0], y=sample[1], size=15, color=\"blue\")\n",
    "    if kmean.labels_[i] == 2:\n",
    "        plot.circle(x=sample[0], y=sample[1], size=15, color=\"purple\")\n",
    "    if kmean.labels_[i] == 3:\n",
    "        plot.circle(x=sample[0], y=sample[1], size=15, color=\"green\")\n",
    "    if kmean.labels_[i] == 4:\n",
    "        plot.circle(x=sample[0], y=sample[1], size=15, color=\"yellow\")  \n",
    "    i += 1\n",
    "\n",
    "# output using given date, normalization with std dev and 5 categories\n",
    "# the last step, I have been trying to evaluate the messy plot and tweek some parameters\n",
    "\n",
    "bokeh.io.show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Tahoma's list of interesting things to experiment with:*\n",
    "\n",
    "* See if clustering would show anything interesting in our dataset.\n",
    "* Make a set out of the category column, and then display its size and elements.\n",
    "* Change the map to display different categories of incident in different colors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
