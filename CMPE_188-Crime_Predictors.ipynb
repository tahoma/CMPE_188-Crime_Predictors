{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# San Francisco Police Department Incident Dataset\n",
    "\n",
    "In this notebook, we'll be exploring the incident report dataset from the San Francisco Police Department using data mining and visualization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Library Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make code development easier and to not clutter the notebook,\n",
    "we'll be referring to definitions from our own notebook-local Python library\n",
    "in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import crime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our raw source dataset can be retrieved from this link: <https://data.sfgov.org/api/views/tmnf-yvry/rows.csv?accessType=DOWNLOAD>.\n",
    "\n",
    "It is larger than the maximum 100 MB file size that github allows.  Until we decide how we're going to filter it and can check it in, just make certain it's downloaded into the same directory as the ipython notebook file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_file = 'SFPD_Incidents_-_from_1_January_2003.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Big Picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is in place, the following code can read in the CSV as a dataframe and report the number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_df = pandas.read_csv(in_file)\n",
    "print(\"Read {} rows x {} columns of input data from '{}'.\".format(in_df.shape[0], in_df.shape[1], in_file))\n",
    "in_df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the list of features from the original dataset so that we can see what kinds of columns we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = list(in_df.columns.values)\n",
    "print(in_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a little bit more information about the types of values in our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_df.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zooming In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks to me like IncidntNum has identical values in one to several adjacent rows through the data set.  Looking further at the details of such groupings of rows, it appears that this column is used as a key to represent several citations, each with unique Category and Descript, for a single incident that happened at the same Date/Time/Location/etc.\n",
    "\n",
    "Given this, it would be interesting to know how many unique values are in this column in order to compare to the total number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incidntnum_values = list(in_df.groupby('IncidntNum').IncidntNum.nunique().index.values)\n",
    "print(\"The 'IncidntNum' column contains {} unique values.\".format(len(incidntnum_values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it is likely to prove useful to know both the count and the actual values for the unique values within each of the Resolution, Category, and Descript columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_values = list(in_df.groupby('Category').Category.nunique().index.values)\n",
    "print(\"The 'Category' column contains {} unique values.\".format(len(category_values)))\n",
    "print(category_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descript_values = list(in_df.groupby('Descript').Descript.nunique().index.values)\n",
    "print(\"The 'Descript' column contains {} unique values.\".format(len(descript_values)))\n",
    "print(descript_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution_values = list(in_df.groupby('Resolution').Resolution.nunique().index.values)\n",
    "print(\"The 'Resolution' column contains {} unique values.\".format(len(resolution_values)))\n",
    "print(resolution_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Cleaning the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the columns in the input dataset need to be cleaned up before we can make much use of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "incident_df = in_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, observe that the 'Date' and 'Time' strings should be converted to a numeric Timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#incident_df['Date'] = pandas.to_datetime(incident_df['Date'])\n",
    "date = incident_df.get_value(0, 'Date')\n",
    "pandas.to_datetime(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Reducing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the available features, we can see several columns that are of little interest to the rest of the work.  So, let's immediately filter for the interesting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = incident_df.filter(items=['Category', 'Descript', 'Date', 'Time', 'Resolution', 'X', 'Y'])\n",
    "print(\"Filtered incident data down to {} rows x {} columns.\".format(filtered_df.shape[0], filtered_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've reduced the dimensionality of our dataset by eliminating uninteresting columns, we can also consider selecting fewer rows.  Looking at the classes of 'Resolution' values, it might make sense to constrain our work over the set of instances that resulted in an arrest and booking and then eliminate the 'Resolution' column.\n",
    "\n",
    "This is a neat way to perform a select to filter rows on a dataframe. Be careful, though, and note that the row indices still match those from the original dataset, which will cause you fits if you're trying to iterate over row indices with the smaller dataset since it no longer has contiguous indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookings_df = filtered_df[filtered_df.Resolution == 'ARREST, BOOKED']\n",
    "bookings_df = bookings_df.drop('Resolution', axis=1)\n",
    "print(\"Bookings data reduced down to {} rows x {} columns.\".format(bookings_df.shape[0], bookings_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the shape of the reduced bookings dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookings_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookings_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookings_df.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: For whatever it's worth, I'm starting to second guess filtering the incidents down to just bookings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Initializing Bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bokeh is a useful Python visualization library that we'll be using multiple times below.  Rather than repeat its initialization or spread it out over multiple locations in the document, I'm giving its initialization an independent section early in the notebook.\n",
    "\n",
    "Also, please note that I had to run the following command before I could get Bokeh plots to display at all.\n",
    "\n",
    "```\n",
    "jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bokeh for visualizations.\n",
    "import bokeh.io\n",
    "import bokeh.models\n",
    "import bokeh.plotting\n",
    "\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Mapping Incidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It would be useful to be able to do visualizations involving\n",
    "layering over a base map of San Francisco based upon coordinates\n",
    "expressed like we have available in the dataset.\n",
    "\n",
    "That said, our bookings dataframe still has several hundred thousand\n",
    "incidents in it, so let's constrain our example visualization down\n",
    "to a single explicit date, using whatever the date of the first\n",
    "incident in our original input data was for arbitrary simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = incident_df.get_value(0, 'Date')\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bookings_on_date = bookings_df[bookings_df.Date == date]\n",
    "df_bookings_on_date = df_bookings_on_date.filter(items=['Category', 'Descript', 'Time', 'X', 'Y'])\n",
    "df_bookings_on_date.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with for map visualization, we've been using Bokeh:\n",
    "<http://bokeh.pydata.org/en/latest/docs/user_guide/geo.html>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh.io.show(crime.map_incidents(df_bookings_on_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: This is just chicken scratch so far.  I should probably move it to a branch for now.\n",
    "\n",
    "NOTE: Before editing, the following snippet originally came from <https://www.kaggle.com/wikaiqi/titanic/titaniclearningqi>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Gaussian Naive Bayes\n",
    "#gaussian = GaussianNB()\n",
    "#gaussian.fit(X_data, Y_data)\n",
    "#Y_pred = gaussian.predict(X_test_kaggle)\n",
    "#acc_gaussian = cross_val_score(gaussian, X_data, Y_data, cv=Kfold)\n",
    "#bcc_gaussian = round(gaussian.score(X_test, Y_test) * 100, 5)\n",
    "\n",
    "#submission = pd.DataFrame({\n",
    "#        \"PassengerId\": test_df[\"PassengerId\"],\n",
    "#        \"Survived\": Y_pred\n",
    "#    })\n",
    "#submission.to_csv('submission_Gassian_Naive_Bayes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*interesting things to experiment with:*\n",
    "\n",
    "* See if clustering would show anything interesting in our dataset.\n",
    "* Change the map to display different categories of incident in different colors.\n",
    "* Write a getDayOfWeekFromDate() helper function in crime library.\n",
    "* Linearize 'Date' and 'Time' fields into a single date/time value (e.g. unix time in seconds since 1970)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
