{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# San Francisco Police Department Incident Dataset\n",
    "\n",
    "In this notebook, we'll be exploring the incident report dataset from the San Francisco Police Department using data mining and visualization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Library Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make code development easier and to not clutter the notebook,\n",
    "we'll be referring to definitions from our own notebook-local Python library\n",
    "in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import crime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our raw source dataset can be retrieved from this link: <https://data.sfgov.org/api/views/tmnf-yvry/rows.csv?accessType=DOWNLOAD>.\n",
    "\n",
    "It is larger than the maximum 100 MB file size that github allows.  Until we decide how we're going to filter it and can check it in, just make certain it's downloaded into the same directory as the ipython notebook file.\n",
    "\n",
    "Once the dataset is in place, the following code can read in the CSV as a dataframe and report the number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incident_file = 'SFPD_Incidents_-_from_1_January_2003.csv'\n",
    "incident_df = pandas.read_csv(incident_file)\n",
    "print(\"Read {} rows x {} columns of incident data from '{}'.\".format(incident_df.shape[0], incident_df.shape[1], incident_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the list of features from the original dataset so that we can see what kinds of columns we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incident_features = incident_df.columns.values\n",
    "print(incident_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a little bit more information about the types of values in our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incident_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incident_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incident_df.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can see the first several rows of our input dataframe like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incident_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is likely to prove useful to know the unique values within each of the Resolution, Category, and Descript columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_values = incident_df.groupby('Category').Category.nunique().index.values\n",
    "print(\"The 'Category' column contains {} unique values.\".format(category_values.size))\n",
    "category_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descript_values = incident_df.groupby('Descript').Descript.nunique().index.values\n",
    "print(\"The 'Descript' column contains {} unique values.\".format(descript_values.size))\n",
    "descript_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution_values = incident_df.groupby('Resolution').Resolution.nunique().index.values\n",
    "print(\"The 'Resolution' column contains {} unique values.\".format(resolution_values.size))\n",
    "resolution_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Cleaning Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the columns in the input dataset need to be cleaned up before we can make much use of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, observe that the 'Date' and 'Time' strings should be converted to a numeric Timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#incident_df['Date'] = pandas.to_datetime(incident_df['Date'])\n",
    "pandas.to_datetime(incident_df['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Reducing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the available features, we can see several columns that are of little interest to the rest of the work.  So, let's immediately filter for the interesting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = incident_df.filter(items=['Category', 'Descript', 'Date', 'Time', 'Resolution', 'X', 'Y'])\n",
    "print(\"Filtered incident data down to {} rows x {} columns.\".format(filtered_df.shape[0], filtered_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've reduced the dimensionality of our dataset by eliminating uninteresting columns, we can also consider selecting fewer rows.  Looking at the classes of 'Resolution' values, it might make sense to constrain our work over the set of instances that resulted in an arrest and booking and then eliminate the 'Resolution' column.\n",
    "\n",
    "This is a neat way to perform a select to filter rows on a dataframe. Be careful, though, and note that the row indices still match those from the original dataset, which will cause you fits if you're trying to iterate over row indices with the smaller dataset since it no longer has contiguous indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookings_df = filtered_df[filtered_df.Resolution == 'ARREST, BOOKED']\n",
    "bookings_df = bookings_df.drop('Resolution', axis=1)\n",
    "print(\"Bookings data reduced down to {} rows x {} columns.\".format(bookings_df.shape[0], bookings_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the shape of the reduced bookings dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookings_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookings_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookings_df.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: For whatever it's worth, I'm starting to second guess filtering the incidents down to just bookings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Initializing Bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bokeh is a useful Python visualization library that we'll be using multiple times below.  Rather than repeat its initialization or spread it out over multiple locations in the document, I'm giving its initialization an independent section early in the notebook.\n",
    "\n",
    "Also, please note that I had to run the following command before I could get Bokeh plots to display at all.\n",
    "\n",
    "```\n",
    "jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bokeh for visualizations.\n",
    "import bokeh.io\n",
    "import bokeh.models\n",
    "import bokeh.plotting\n",
    "\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Mapping Incidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It would be useful to be able to do visualizations involving\n",
    "layering over a base map of San Francisco based upon coordinates\n",
    "expressed like we have available in the dataset.\n",
    "\n",
    "That said, our bookings dataframe still has several hundred thousand\n",
    "incidents in it, so let's constrain our example visualization down\n",
    "to a single explicit date, using whatever the date of the first\n",
    "incident in our original input data was for arbitrary simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = incident_df.get_value(0, 'Date')\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bookings_on_date = bookings_df[bookings_df.Date == date]\n",
    "df_bookings_on_date = df_bookings_on_date.filter(items=['Category', 'Descript', 'Time', 'X', 'Y'])\n",
    "df_bookings_on_date.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with for map visualization, we've been using Bokeh:\n",
    "<http://bokeh.pydata.org/en/latest/docs/user_guide/geo.html>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh.io.show(crime.map_incidents(df_bookings_on_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before editing, the following snippet originally came from <https://www.kaggle.com/wikaiqi/titanic/titaniclearningqi>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "#gaussian = GaussianNB()\n",
    "#gaussian.fit(X_data, Y_data)\n",
    "#Y_pred = gaussian.predict(X_test_kaggle)\n",
    "#acc_gaussian = cross_val_score(gaussian, X_data, Y_data, cv=Kfold)\n",
    "#bcc_gaussian = round(gaussian.score(X_test, Y_test) * 100, 5)\n",
    "\n",
    "#submission = pd.DataFrame({\n",
    "#        \"PassengerId\": test_df[\"PassengerId\"],\n",
    "#        \"Survived\": Y_pred\n",
    "#    })\n",
    "#submission.to_csv('submission_Gassian_Naive_Bayes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Clustering Incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# in order to use K-means, the inputs must be numerical, so we have to discretize the category input\n",
    "# found this post off stackoverflow helpful \n",
    "# http://stackoverflow.com/questions/34915813/convert-text-columns-into-numbers-in-sklearn \n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "test_series = df_bookings_on_date[df_bookings_on_date.columns[0:2]].apply(le.fit_transform)\n",
    "\n",
    "print(test_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the input data before using kmeans\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "normalized_df = (test_series-test_series.mean())/test_series.std()\n",
    "\n",
    "# TMT: it might make more sense to cluster by location...\n",
    "#normalized_df = df_bookings_on_date[df_bookings_on_date.columns[3:5]]\n",
    "\n",
    "# remove the column names by transforming dataframe into matrix\n",
    "testdata = normalized_df.as_matrix(columns=None)\n",
    "\n",
    "print(testdata)\n",
    "#perform k-means analysis on the reduced data set\n",
    "\n",
    "kmean = KMeans(n_clusters=5) \n",
    "\n",
    "kmean.fit(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = bokeh.plotting.figure(\n",
    "    width = 500,\n",
    "    height = 500,\n",
    "    title = 'CrimeStoppers',\n",
    "    x_axis_label = \"category\",\n",
    "    y_axis_label = \"descript\"\n",
    ")\n",
    "\n",
    "#plot centroid / cluster center / group mean for each group\n",
    "\n",
    "clus_xs = []\n",
    "\n",
    "clus_ys = []\n",
    "\n",
    "#we get the  cluster x / y values from the k-means algorithm\n",
    "\n",
    "for entry in kmean.cluster_centers_:\n",
    "\n",
    "   clus_xs.append(entry[0])\n",
    "\n",
    "   clus_ys.append(entry[1])\n",
    "\n",
    "#the cluster center is marked by a circle, with a cross in it\n",
    "\n",
    "plot.circle_cross(\n",
    "    x=clus_xs,\n",
    "    y=clus_ys,\n",
    "    size=40,\n",
    "    fill_alpha=0,\n",
    "    line_width=2,\n",
    "    color=['red', 'blue', 'purple', 'green', 'yellow']\n",
    ")\n",
    "\n",
    "plot.text(text = ['something', 'other', 'another', 'yet', 'more'], x=clus_xs, y=clus_ys, text_font_size='30pt')\n",
    "\n",
    "i = 0 #counter\n",
    "\n",
    "#begin plotting each petal length / width\n",
    "\n",
    "#We get our x / y values from the original plot data.\n",
    "\n",
    "#The k-means algorithm tells us which 'color' each plot point is,\n",
    "\n",
    "#and therefore which group it is a member of.\n",
    "\n",
    "for sample in testdata:\n",
    "\n",
    "    #\"labels_\" tells us which cluster each plot point is a member of\n",
    "    if kmean.labels_[i] == 0:\n",
    "        plot.circle(x=sample[0], y=sample[1], size=15, color=\"red\")\n",
    "    if kmean.labels_[i] == 1:\n",
    "        plot.circle(x=sample[0], y=sample[1], size=15, color=\"blue\")\n",
    "    if kmean.labels_[i] == 2:\n",
    "        plot.circle(x=sample[0], y=sample[1], size=15, color=\"purple\")\n",
    "    if kmean.labels_[i] == 3:\n",
    "        plot.circle(x=sample[0], y=sample[1], size=15, color=\"green\")\n",
    "    if kmean.labels_[i] == 4:\n",
    "        plot.circle(x=sample[0], y=sample[1], size=15, color=\"yellow\")  \n",
    "    i += 1\n",
    "\n",
    "# output using given date, normalization with std dev and 5 categories\n",
    "# the last step, I have been trying to evaluate the messy plot and tweek some parameters\n",
    "\n",
    "bokeh.io.show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*interesting things to experiment with:*\n",
    "\n",
    "* See if clustering would show anything interesting in our dataset.\n",
    "* Change the map to display different categories of incident in different colors.\n",
    "* Write a getDayOfWeekFromDate() helper function in crime library.\n",
    "* Linearize 'Date' and 'Time' fields into a single date/time value (e.g. unix time in seconds since 1970)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
